{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FOLDER = 'wikispeedia_paths-and-graph/'\n",
    "LINKS_DATA = PATH_FOLDER + \"links.tsv\"\n",
    "PATH_FINISHED_DATA = PATH_FOLDER + \"paths_finished.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv(LINKS_DATA, sep='\\t', header=None, names=[\"linkSource\", 'linkTarget'], comment='#')\n",
    "path_finished = pd.read_csv(PATH_FINISHED_DATA, sep='\\t', header=None, names=['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating'], comment='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import unquote\n",
    "\n",
    "def unquote_df(df, columns):\n",
    "    '''Inputs:\n",
    "            df: panda dataframe\n",
    "            columns: string or string array containing column names to be url-decoded\n",
    "        Return:\n",
    "            panda dataframe with url-decoded column names\n",
    "    '''\n",
    "    for column in columns:\n",
    "        N = len(df[column])\n",
    "        for i in range(N):\n",
    "            df.loc[i,column] = unquote(df.loc[i,column])\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_character(headline, character):\n",
    "    '''Inputs:\n",
    "            headline: string\n",
    "            list_words: string\n",
    "        Return:\n",
    "            1 if the character is in the headline\n",
    "            0 otherwise\n",
    "    '''\n",
    "    if character in headline: #\n",
    "        return 1 \n",
    "    else:\n",
    "         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = unquote_df(links, ['linkSource','linkTarget'])\n",
    "path_finished = unquote_df(path_finished, ['path'])\n",
    "#path_finished['back'] = path_finished['path'].apply(lambda x : check_character(x, '<'))\n",
    "#path_finished = path_finished[path_finished['back']==0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de maintenant, uniquement mon code (avant: recopie pour run notebook proprement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a directed graph with the wikipedia articles to perform a fictional random walk on it using the pagerank alorithm, which will be our normalization for the distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikigraph=nx.DiGraph()\n",
    "e=zip(links['linkSource'], links['linkTarget'])\n",
    "Wikigraph.add_edges_from(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abbot', 'Dante_Alighieri', 'Durham_Cathedral', 'England', 'Great_Britain', 'Hebrew_language', 'Julius_Caesar', 'Middle_Ages', 'Music', 'Paul_of_Tarsus', 'Season', 'Virgil']\n"
     ]
    }
   ],
   "source": [
    "#nx.draw(Wikigraph)\n",
    "#plt.show()\n",
    "print(list(Wikigraph.successors('Bede')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00956554538310564\n",
      "0.00956554538310564\n"
     ]
    }
   ],
   "source": [
    "GooglePageRank=nx.pagerank(Wikigraph, alpha=0.85)\n",
    "print(GooglePageRank.get('United_States')) \n",
    "print(max(GooglePageRank.values())) #we see that the maximum value is reached for the US, which means it will be reached more times than any other article on a RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(string):\n",
    "    return list(string.split(';'))\n",
    "\n",
    "path_finished['path']=path_finished['path'].apply(lambda x:convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_a(article, goal, paths):\n",
    "    '''This function counts the number of times an article was encountered on all paths with a specified goal\n",
    "    Input: article encountered, goal of the paths, object with all paths\n",
    "    Output: number of times the article was encountered summed on all paths with same given goal'''\n",
    "    count=0\n",
    "    for path in paths:\n",
    "        if path[-1]==goal: #checks that the last element is the correct goal\n",
    "            for art in path:\n",
    "                if art==article:\n",
    "                    count+=1\n",
    "    return count\n",
    "#note that we decide to count all the times an article could appear in the same path, because it would mean that it has a more significant value than if it just appeared once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_aprime(aprime, article, goal, paths):\n",
    "    count=0\n",
    "    if aprime=='<' or article=='<': #the comeback sign is not part of the count\n",
    "        return 0\n",
    "\n",
    "    for path in paths:\n",
    "        if path[-1]==goal:\n",
    "            pathcycle=cycle(path)\n",
    "            next_art=next(pathcycle)\n",
    "            for _ in range(len(path)-1):\n",
    "                art, next_art=next_art, next(pathcycle)\n",
    "                if art==article:\n",
    "                    if next_art==aprime:\n",
    "                            count+=1    \n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT&T', 'United_States', 'Agriculture', 'Vegetable']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_finished['path'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "count=count_a('United_States','South_America',path_finished['path'])\n",
    "count #seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "count_successor=count_aprime('Nature','Science','Rainbow',path_finished['path'])\n",
    "print(count_successor)\n",
    "#note that '<' is not included in the Wikigraph\n",
    "#ATTENTION: what to do when aprime then <? remove them?\n",
    "#seems to work otherwise too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_click_probability(aprime, article, goal, paths, alpha=0):\n",
    "    '''Calculates the posterior click probability to reach an article given the previous article and the goal, after seeing all the data\n",
    "    Input: aprime: article on which the proba is done, article:previous article, goal:final article, paths:evaluated on all those paths,\n",
    "    alpha:Dirichlet parameter representing initial confidence in uniform prior distribution\n",
    "    Output: posterior click probability for aprime, given article and goal'''\n",
    "    \n",
    "    k_a=len(list(Wikigraph.successors(article)))#number of out-degree links for article\n",
    "    if count_a(article, goal, paths)!=0 and count_aprime(aprime, article, goal, paths):\n",
    "        proba=(count_aprime(aprime, article, goal, paths)+alpha)/(count_a(article, goal, paths)+alpha*k_a)\n",
    "    else: proba=1 #because of the log\n",
    "\n",
    "    return proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#proba=posterior_click_probability('Time','14th_century','Rainbow',path_finished['path'])\n",
    "proba=posterior_click_probability('Dutch_language','Darth_Vader','Roman_Catholic_Church',path_finished['path'])\n",
    "proba\n",
    "#seems to work as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to determine alpha parameter???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_distance(a_i, goal, path):\n",
    "    sum=0\n",
    "    i=-1\n",
    "    if a_i!=goal:\n",
    "        for a in path:\n",
    "            if a==a_i:\n",
    "                i=path.index(a)\n",
    "        if i==-1:\n",
    "            print('Error: The article looked for is not in the path')\n",
    "            return 0\n",
    "\n",
    "        pathcycle=cycle(path[i:])\n",
    "        next_a=next(pathcycle)\n",
    "        for _ in range(len(path)-1):\n",
    "            a_i, next_a=next_a, next(pathcycle)\n",
    "            if a_i!='<':\n",
    "                p=posterior_click_probability(next_a, a_i, goal, path_finished['path'])\n",
    "                sum-=np.log(p)\n",
    "    \n",
    "    return sum/(-np.log(GooglePageRank.get(goal)))\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The article looked for is not in the path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_distance('15th_century','African_slave_trade',path_finished['path'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_distance(article, goal,paths):\n",
    "    dist=0\n",
    "    m=0\n",
    "    pat=[]\n",
    "\n",
    "    for path in paths:\n",
    "        if path[-1]==goal:\n",
    "            for a in path:\n",
    "                if a==article:\n",
    "                    dist+=path_distance(article, goal, path)\n",
    "                    m+=1\n",
    "                    pat.append(path)\n",
    "\n",
    "    if m==0: \n",
    "        print('Error, no such path exists')\n",
    "        return 0,0,0\n",
    "\n",
    "    return dist/m, m, pat\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, no such path exists\n",
      "Error, no such path exists\n",
      "Error, no such path exists\n",
      "0.0 1\n"
     ]
    }
   ],
   "source": [
    "sdist1, m1, pat1=semantic_distance('Noam_Chomsky','Linguistics',path_finished['path'])\n",
    "#sdist2, m2, pat2=semantic_distance('Noam_Chomsky','Communication',path_finished['path'])\n",
    "#sdist3, m3, pat3=semantic_distance('Noam_Chomsky','Language',path_finished['path'])\n",
    "#sdist4, m4, pat4=semantic_distance('Noam_Chomsky','Rainbow',path_finished['path'])\n",
    "\n",
    "print(sdist1, m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "Wikigraph.has_node('Minneapolis')\n",
    "print(pat1,pat4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wikigraph.has_node('Language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Belarus', 'Bertrand_Russell', 'Cape_Town', 'Chinese_language', 'Computer_science', 'Dutch_language', 'English_language', 'Fascism', 'French_language', 'German_language', 'Hebrew_language', 'Human_rights', 'Jerry_Fodor', 'John_Locke', 'Language', 'Linguistics', 'Mass_media', 'Nazism', 'New_Delhi', 'Philosophy', 'Philosophy_of_mind', 'Propaganda', 'Psychology', 'Socialism', 'Spanish_language', 'Ukraine', 'United_States', 'University_of_Cambridge', 'Vietnam_War']\n"
     ]
    }
   ],
   "source": [
    "print(list(Wikigraph.successors('Noam_Chomsky')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [242], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m noam_data \u001b[39m=\u001b[39m path_finished[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m][path_finished[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49misin([\u001b[39m'\u001b[39;49m\u001b[39mNoam_Chomsky\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49many()]\n",
      "File \u001b[1;32mc:\\Users\\zoeje\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zoeje\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\zoeje\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\core\\indexes\\range.py:395\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m--> 395\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m    396\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_loc(key, method\u001b[39m=\u001b[39mmethod, tolerance\u001b[39m=\u001b[39mtolerance)\n",
      "\u001b[1;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "noam_data = path_finished['path'][path_finished['path'].isin(['Noam_Chomsky']).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff78d141b9f8c348e034749b07489e70aca0772d9c1402feaf5c8610ccc21981"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
